{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Assignment Question"
      ],
      "metadata": {
        "id": "H6fc4vV78dEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. What is a Decision Tree, and how does it work?"
      ],
      "metadata": {
        "id": "zoVQCug-8i77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> A Decision Tree is a non-parametric supervised learning algorithm that can be used for both classification and regression tasks.  It works by recursively splitting the data based on features to create a tree-like structure."
      ],
      "metadata": {
        "id": "AXnwRAVC_I5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are impurity measures in Decision Trees\n"
      ],
      "metadata": {
        "id": "cDXUXA3c9URW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Impurity measures in Decision Trees are used to determine how homogeneous a set of data is with respect to the target variable. In other words, they quantify how mixed the classes are within a node. The goal of the decision tree algorithm is to find splits that reduce the impurity of the resulting child nodes."
      ],
      "metadata": {
        "id": "ty-ZqAON_Oxm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the mathematical formula for Gini Impurity\n"
      ],
      "metadata": {
        "id": "_-YQBt5u9WFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Gini Impurity: This measures the probability of misclassifying a randomly chosen element from the dataset if it were randomly labeled according to the distribution of classes in the node. A Gini impurity of 0 means the node is perfectly pure (all samples belong to the same class)"
      ],
      "metadata": {
        "id": "BqVUKker_Tfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the mathematical formula for Entropy\n"
      ],
      "metadata": {
        "id": "lJ3YySmd9X2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Entropy: This is a measure of the randomness or disorder in a set of data. In the context of decision trees, it measures the uncertainty in a node based on the distribution of classes. A lower entropy value indicates less uncertainty and a more pure node."
      ],
      "metadata": {
        "id": "t9KImX4X_XtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Information Gain, and how is it used in Decision Trees?"
      ],
      "metadata": {
        "id": "Eu4-U1DJ9Zjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Based on the code you provided and the concept of impurity measures, Information Gain is a metric used in Decision Trees to determine the effectiveness of a split. It quantifies how much the entropy (or impurity) of the data is reduced after splitting a node based on a particular feature."
      ],
      "metadata": {
        "id": "93F3Pz3J_dqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the difference between Gini Impurity and Entropy\n"
      ],
      "metadata": {
        "id": "lgcOjr1M9bsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Both Gini Impurity and Entropy are impurity measures used in Decision Trees to evaluate the homogeneity of a node. While they serve the same purpose, there are some key differences:\n",
        "\n",
        "Mathematical Formula:\n",
        "\n",
        "Gini Impurity: The formula for Gini Impurity is $\\sum_{i=1}^{C} p(i) * (1 - p(i))$, where $C$ is the number of classes and $p(i)$ is the proportion of samples belonging to class $i$ in the node. [1]\n",
        "Entropy: The formula for Entropy is $-\\sum_{i=1}^{C} p(i) * \\log_2(p(i))$, where $C$ is the number of classes and $p(i)$ is the proportion of samples belonging to class $i$ in the node."
      ],
      "metadata": {
        "id": "mJLYvZOP_mub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is the mathematical explanation behind Decision Trees"
      ],
      "metadata": {
        "id": "2kkGc3Uy9dkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> These models work by splitting data into subsets based on feature and this splitting is called as decision making and each leaf node tells us prediction. This splitting creates a tree-like structure. They are easy to interpret and visualize for understanding the decision-making process."
      ],
      "metadata": {
        "id": "GIcRyetC_1mA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Pre-Pruning in Decision Trees"
      ],
      "metadata": {
        "id": "GQZFLo4d9fpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> There are two main types of decision tree pruning: Pre-Pruning and Post-Pruning. Sometimes, the growth of the decision tree can be stopped before it gets too complex, this is called pre-pruning."
      ],
      "metadata": {
        "id": "bu9G5wjp_9R-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is Post-Pruning in Decision Trees"
      ],
      "metadata": {
        "id": "17-4H1rG9jNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Post-pruning, also known as pruning the tree, consists of constructing the full tree and thereafter eliminating nodes that do not contribute significantly to predictive power. This is usually accomplished by methods such as cost-complexity pruning."
      ],
      "metadata": {
        "id": "WiPsliBaAGd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the difference between Pre-Pruning and Post-Pruning"
      ],
      "metadata": {
        "id": "De0oPNv99k29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Pre-pruning involves pruning the tree during its construction. The method assesses at each node if dividing the node further will enhance the overall performance on the validation data. If not, the node is designated as a leaf without additional division.\n",
        "\n",
        "Post-pruning, also known as pruning the tree, consists of constructing the full tree and thereafter eliminating nodes that do not contribute significantly to predictive power. This is usually accomplished by methods such as cost-complexity pruning."
      ],
      "metadata": {
        "id": "sJmClRn8AMAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is a Decision Tree Regressor"
      ],
      "metadata": {
        "id": "SXmtpL3e9mgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> A Decision Tree Regressor is a machine learning model used for predicting continuous values. It works by splitting the data into subsets based on the values of the input features, creating a tree-like structure where each internal node represents a decision based on a feature, and each leaf node represents a predicted value."
      ],
      "metadata": {
        "id": "obWpt14YAUjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What are the advantages and disadvantages of Decision Trees"
      ],
      "metadata": {
        "id": "BlYVcXMG9oSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Advantages:\n",
        "\n",
        "Easy to understand and interpret:-  Decision trees provide a clear visual representation of the decision-making process, making them easy to understand for both technical and non-technical audiences.\n",
        "Can handle both numerical and categorical data:-  Decision trees can handle different types of data without requiring extensive preprocessing.\n",
        "Require little data preparation:-  Unlike some other algorithms, decision trees do not require feature scaling or normalization.\n",
        "Non-parametric:-  They don't make assumptions about the underlying data distribution.\n",
        "Can model non-linear relationships: Decision trees can capture complex non-linear relationships between features and the target variable.\n",
        "* Disadvantages:\n",
        "\n",
        "Prone to overfitting:-  Decision trees can easily overfit the training data, especially when they are grown to a large depth. This can lead to poor performance on unseen data.\n",
        "Instability:-  Small changes in the data can lead to significant changes in the tree structure.\n",
        "Bias towards features with more levels: Decision trees may favor features with a larger number of categories or continuous values, as they can potentially create more splits and thus appear to offer higher Information Gain.\n",
        "Can create biased trees:-  If there is a dominant class in the dataset, the tree may become biased towards that class.\n",
        "Difficult to handle missing values:-  While some strategies exist, handling missing values can be challenging in decision trees.\n",
        "Computational cost:-  Building a large decision tree can be computationally expensive, especially for large datasets."
      ],
      "metadata": {
        "id": "XOVptNdYAiMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. How does a Decision Tree handle missing values"
      ],
      "metadata": {
        "id": "43aYLr6s9pyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Handling missing values in Decision Trees can be approached in a few ways, although it's not always straightforward or explicitly built into every Decision Tree implementation"
      ],
      "metadata": {
        "id": "eby0FxLXA6QF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How does a Decision Tree handle categorical features"
      ],
      "metadata": {
        "id": "glh9N9fq9rii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Decision Trees handle categorical features by considering the different categories as potential splits. The way this is done can vary slightly depending on whether the categorical feature is nominal (categories with no inherent order"
      ],
      "metadata": {
        "id": "4OU9PebYBBgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What are some real-world applications of Decision Trees?"
      ],
      "metadata": {
        "id": "sSkDpXl89tWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Decision Trees are a versatile algorithm with applications in various real-world scenarios across different domains. Here are some examples:\n",
        "\n",
        "Medical Diagnosis: Decision trees can be used to assist in diagnosing diseases based on patient symptoms and medical history. The tree can help doctors make decisions about potential diagnoses by guiding them through a series of questions.\n",
        "\n",
        "Credit Risk Assessment: Financial institutions use decision trees to assess the creditworthiness of loan applicants. The tree can evaluate factors like income, debt-to-income ratio, and credit history to predict the likelihood of default."
      ],
      "metadata": {
        "id": "B8B3lO69BIQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practical Questions"
      ],
      "metadata": {
        "id": "rsHSM9Xc9uJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy"
      ],
      "metadata": {
        "id": "Nrpo6TMe9ziQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the the above ques.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "szQJpy2Y-2Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances\n"
      ],
      "metadata": {
        "id": "W1C8PEYS-S9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the the above ques.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini')\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "feature_importances = clf.feature_importances_\n",
        "\n",
        "for feature, importance in zip(iris.feature_names, feature_importances):\n",
        "    print(f\"{feature}: {importance}\")"
      ],
      "metadata": {
        "id": "oBJWwKJO-9MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy\n"
      ],
      "metadata": {
        "id": "rATKZqTO-Ved"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the the above ques.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='entropy')\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "2jvbY2lU-9zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE)\n"
      ],
      "metadata": {
        "id": "LZX2JUbf-YQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the the above ques.\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "regressor = DecisionTreeRegressor()\n",
        "\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n"
      ],
      "metadata": {
        "id": "h67kf6nT--NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz"
      ],
      "metadata": {
        "id": "sWpc3Aig-amG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the the above ques.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.model_selection import train_test_split\n",
        "import graphviz\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "dot_data = export_graphviz(clf, out_file=None,\n",
        "                           feature_names=iris.feature_names,\n",
        "                           class_names=iris.target_names,\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"iris_decision_tree\")\n",
        "graph.view()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SMG1wRnN--qp",
        "outputId": "3a673d08-5dce-4cc8-a6c8-a8e499d83880"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'iris_decision_tree.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree\n"
      ],
      "metadata": {
        "id": "FNWllvVR-c1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the the above ques.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf_full_tree = DecisionTreeClassifier()\n",
        "clf_limited_depth = DecisionTreeClassifier(max_depth=3)\n",
        "\n",
        "clf_full_tree.fit(X_train, y_train)\n",
        "clf_limited_depth.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred_full_tree = clf_full_tree.predict(X_test)\n",
        "y_pred_limited_depth = clf_limited_depth.predict(X_test)\n",
        "\n",
        "accuracy_full_tree = accuracy_score(y_test, y_pred_full_tree)\n",
        "accuracy_limited_depth = accuracy_score(y_test, y_pred_limited_depth)\n",
        "\n",
        "print(\"Accuracy with Full Tree:\", accuracy_full_tree)\n",
        "print(\"Accuracy with Limited Depth Tree:\", accuracy_limited_depth)"
      ],
      "metadata": {
        "id": "0maPL86_-_H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree\n"
      ],
      "metadata": {
        "id": "HA3AhGVs-f2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the the above ques.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf_default = DecisionTreeClassifier()\n",
        "clf_min_samples_split_5 = DecisionTreeClassifier(min_samples_split=5)\n",
        "\n",
        "clf_default.fit(X_train, y_train)\n",
        "clf_min_samples_split_5.fit(X_train, y_train)\n",
        "\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "y_pred_min_samples_split_5 = clf_min_samples_split_5.predict(X_test)\n",
        "\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "accuracy_min_samples_split_5 = accuracy_score(y_test, y_pred_min_samples_split_5)\n",
        "\n",
        "print(\"Accuracy with Default Tree:\", accuracy_default)"
      ],
      "metadata": {
        "id": "48oTMVo2-_lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data\n"
      ],
      "metadata": {
        "id": "jJz8jNM3-h3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the the above ques.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf_unscaled = DecisionTreeClassifier()\n",
        "clf_scaled = DecisionTreeClassifier()\n",
        "\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(\"Accuracy with Unscaled Data:\", accuracy_unscaled)"
      ],
      "metadata": {
        "id": "XoxvYMiC_AJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification\n"
      ],
      "metadata": {
        "id": "1CFpj-RE-j-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the the above ques.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "base_clf = DecisionTreeClassifier()\n",
        "\n",
        "ovr_clf = OneVsRestClassifier(base_clf)\n",
        "\n",
        "ovr_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = ovr_clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy with One-vs-Rest strategy:\", accuracy)"
      ],
      "metadata": {
        "id": "a-VFA-ee_AqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train a Decision Tree Classifier and display the feature importance scores"
      ],
      "metadata": {
        "id": "vIlJYfyV-mHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the the above ques.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "feature_importances = clf.feature_importances_\n",
        "\n",
        "for feature, importance in zip(iris.feature_names, feature_importances):\n",
        "    print(f\"{feature}: {importance}\")"
      ],
      "metadata": {
        "id": "uMRK4Usx_BMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree\n"
      ],
      "metadata": {
        "id": "-9sjTzc2-oM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the the above ques.\n",
        "\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "regressor_full_tree = DecisionTreeRegressor()\n",
        "regressor_limited_depth = DecisionTreeRegressor(max_depth=5)\n",
        "\n",
        "regressor_full_tree.fit(X_train, y_train)\n",
        "regressor_limited_depth.fit(X_train, y_train)\n",
        "\n",
        "y_pred_full_tree = regressor_full_tree.predict(X_test)\n",
        "y_pred_limited_depth = regressor_limited_depth.predict(X_test)\n",
        "\n",
        "mse_full_tree = mean_squared_error(y_test, y_pred_full_tree)\n",
        "mse_limited_depth = mean_squared_error(y_test, y_pred_limited_depth)\n",
        "\n",
        "print(\"Mean Squared Error with Full Tree:\", mse_full_tree)\n",
        "print(\"Mean Squared Error with Limited Depth Tree:\", mse_limited_depth)"
      ],
      "metadata": {
        "id": "RC_M97ZA_Bn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy\n"
      ],
      "metadata": {
        "id": "gT2amxtO-qjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the the above ques.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "ccp_alphas = []\n",
        "ccp_accuracies = []\n",
        "\n",
        "for i in range(1, 10):\n",
        "    clf = DecisionTreeClassifier(ccp_alpha=i/10)\n",
        "    clf.fit(X_train, y_train)\n",
        "    ccp_alphas.append(i/10)\n",
        "    ccp_accuracies.append(clf.score(X_test, y_test))\n",
        "\n",
        "plt.plot(ccp_alphas, ccp_accuracies)\n",
        "\n",
        "plt.xlabel('CCP Alpha')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Effect of CCP Alpha on Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ob_-ZTrO_CEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision, Recall, and F1-Score\n"
      ],
      "metadata": {
        "id": "wWGT8FDd-sb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the the above ques.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create an instance of the DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1_score = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-cNyPxW_Cj5",
        "outputId": "a6033cfd-ccbb-4b13-c285-a4222d766bdd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn."
      ],
      "metadata": {
        "id": "3EfDD7Lk-wYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the the above ques.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "90pxA2rk_DDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split."
      ],
      "metadata": {
        "id": "0u1xlb9I-yQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the the above ques.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "y = iris.target\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
        "\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n"
      ],
      "metadata": {
        "id": "iCxaU7fd_Dhl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}